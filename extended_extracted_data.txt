Structured Data:
{
  "@context": "https://schema.org",
  "@type": "Article",
  "name": "Altera",
  "url": "https://en.wikipedia.org/wiki/Altera",
  "sameAs": "http://www.wikidata.org/entity/Q438294",
  "mainEntity": "http://www.wikidata.org/entity/Q438294",
  "author": {
    "@type": "Organization",
    "name": "Contributors to Wikimedia projects"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Wikimedia Foundation, Inc.",
    "logo": {
      "@type": "ImageObject",
      "url": "https://www.wikimedia.org/static/images/wmf-hor-googpub.png"
    }
  },
  "datePublished": "2004-03-10T02:02:53Z",
  "dateModified": "2024-08-19T03:58:57Z",
  "image": "https://upload.wikimedia.org/wikipedia/commons/7/71/Alteraheadquarters.jpg",
  "headline": "semiconductor company"
}

Headings:

.join(map(clean_text, data['headings'])) + '

'Paragraphs:
Altera Corporation is a manufacturer of programmable logic devices (PLDs) headquartered in San Jose, California. It was founded in 1983 and acquired by Intel in 2015 before becoming independent once again in 2024 as a company focused on development of Field-Programmable Gate Array (FPGA) technology and system on a chip FPGAs.
The company was founded in 1983 by semiconductor veterans Rodney Smith, Robert Hartmann, James Sansbury, and Paul Newhagen with $500,000 in seed money. The name of the company was a play on "alterable", the type of chips the company created. In 1988, Altera became a public company via an initial public offering (IPO).
On December 28, 2015, the company was acquired by Intel and became a newly formed business unit called Programmable Solutions Group (PSG). In October 2023, Intel announced it would be spinning off PSG into a separate company at the start of 2024, while maintaining majority ownership and intending to seek an IPO within three years. In February 2024, Intel announced that the newly independent company would reestablish the Altera name and branding.
The main product lines from Altera are the Agilex FPGA product lines, and their predecessors: the high-end Stratix series, mid-range Arria series, and lower-cost Cyclone series; as well as the MAX series non-volatile FPGAs.
Altera and its partners offer an array of semiconductor intellectual property cores that serve as building blocks that design engineers can drop into their system designs to perform specific functions. IP cores eliminate some of the time-consuming tasks of creating every block in a design from scratch. In 2000, Altera acquired Designpro and Northwest Logic, providers of IP cores, in order to expand its design capabilities and move towards delivery of complete system-on-chip solutions.
Beginning in December 2012, the company announced the shipment of its first system on a chip FPGA devices using a fully depleted silicon on insulator (FDSOI) 28nm chip manufacturing process. These are the Cyclone V SoC devices, which have a dual-core ARM architecture Cortex-A9 processor system with FPGA logic on a single chip. These devices integrated FPGAs with full hard processor systems based around ARM architecture onto a single device. As of 2024, the majority of Altera's FPGA devices are available as an SoC variant with an ARM hard processor system integrated with the FPGA as a single system on a chip.
These SoCs are targeted for use in wireless communications, industrial, video surveillance, automotive and medical equipment markets. With these SoCs devices, users were able to create custom field-programmable SoC variants for power, board space, performance and cost optimization.
Cyclone V SoC, Arria V SoC and Arria 10 SoC product families are system on a chip FPGAs based upon a hard ARM Cortex-A9 dual-core processor system.
Stratix 10 SoC and Agilex 7 SoC product families are system on a chip FPGAs based upon a hard ARM Cortex-A53 quad-core processor system.
The Agilex 5 SoC product family are system on a chip FPGAs based upon a hard ARM Cortex-A76/A55 quad-core processor system.
Altera offers the Nios V embedded soft processor cores based on the RISC-V instruction set architecture. Previously Altera had offered their own proprietary Nios II embedded soft processor, the Freescale ColdFire v1 core, and the ARM Cortex-M1 processor.
All of Altera's devices are supported by a common design environment, the Quartus Prime design software, which is a multi-platform development environment that includes various tools needed to design FPGAs, SoC FPGAs, and CPLDs.
In May 2013, Altera made available SDK for OpenCL, enabling software programmers to access the high-performance capabilities of programmable logic devices.
Altera also support high-level synthesis using SYCL extensions to ANSI C/C++.
In 1984, the company formed a long-running design partnership with Intel. In 1994, Altera acquired the PLD business of Intel for $50 million.
In February 2013, Altera announced an agreement to use Intel's foundry services to produce its 14-nm node for the future manufacturing of its FPGAs, based on Intel's 14-nm tri-gate transistor technology, in place of Altera's ongoing agreement with TSMC. The Stratix 10 product family was the first such product line.
In December 2015, Intel acquired Altera for $16.7 billion in cash. Altera became Intel's newly formed business unit called the Programmable Solutions Group (PSG).
In October 2023, Intel announced it would be spinning off PSG into a separate company at the start of 2024, while maintaining majority ownership and intending to seek an IPO within three years. In February 2024, Intel announced that the newly independent company would reestablish the Altera name and branding.
On June 21, 2006, after an investigation by the U.S. Securities and Exchange Commission, the company restated its financial results from 1996 to 2005 to correct accounting errors related to options backdating. The chief financial officer of the company resigned. Altera filed a petition to overturn related regulations but was, under Intel, denied in 2020.

Tables:
Altera CorporationHeadquarters in San Jose, CaliforniaCompany typeSubsidiaryIndustryIntegrated circuitsFoundedJune 1983; 41 years ago (June 1983)HeadquartersSan Jose, California, United StatesKey peopleSandra L. Rivera (CEO)ProductsFPGAsCPLDsEmbedded systems ASICsRevenue $1.932 billion (2014)Net income $472 million (2014)Total assets $5.674 billion (2014)Total equity $3.285 billion (2014)Number of employees3,091 (2014)ParentIntelWebsitealtera.comFootnotes / references
vteIntelSubsidiaries 3Dlabs Altera Intel Security Mobileye Recon Instruments Virtutech Wind River Systems Xircom Joint venture4Group Holdings (50% owned by Vantiva)Products 3D XPoint Accounts & SSO Amplify Tablet Advanced Programmable Interrupt Controller Cache Acceleration Software Client Initiated Remote Access Direct Media Interface Flexible Display Interface Hella Zippy Intel 1103 Intel AZ210 Intel Clear Video Intel Display Power Saving Technology Intel Modular Server System Intel Quick Sync Video Intel Reader Intel system development kit Intel Upgrade Service Intel740 InTru3D IXP1200 OFono Omni-Path Performance acceleration technology Shooting Star SSDs (X25-M) Stable Image Platform Program Virtual 8086 mode WiDi x86 vteIntel processorsLists Processors Atom Celeron Pentium Pro II III 4 D M Core 2 i3 i5 i7 i9 M Xeon Quark Itanium Microarchitectures Chipsets MicroarchitecturesIA-32 (32-bit x86) P5 P6 P6 variant (Pentium M) P6 variant (Enhanced Pentium M) NetBurst x86-64 (64-bit) Core Penryn Nehalem Westmere Sandy Bridge Ivy Bridge Haswell Broadwell Skylake Cannon Lake Sunny Cove Cypress Cove Willow Cove Golden Cove x86 ULV Bonnell Saltwell Silvermont Goldmont Goldmont Plus Tremont Gracemont Current productsx86-64 (64-bit) Atom Celeron Pentium Core 10th gen 11th gen 12th gen 13th gen 14th gen Xeon DiscontinuedBCD oriented (4-bit) 4004 (1971) 4040 (1974) pre-x86 (8-bit) 8008 (1972) 8080 (1974) 8085 (1977) Early x86 (16-bit) 8086 (1978) 8088 (1979) 80186 (1982) 80188 (1982) 80286 (1982) x87 (external FPUs) 8/16-bit databus 8087 (1980) 16-bit databus 80C187 80287 80387SX 32-bit databus 80387DX 80487 IA-32 (32-bit x86) i386 SX 376 EX i486 SX DX2 DX4 SL RapidCAD OverDrive A100/A110 Atom CE SoC Celeron (1998) M D (2004) Pentium Original i586 OverDrive Pro II III 4 M Dual-Core Core Xeon P6-based NetBurst-based Core-based Quark Tolapai x86-64 (64-bit) Atom SoC CE Celeron D Dual-Core Pentium 4 D Extreme Edition Dual-Core Core 2 1st gen 2nd gen 3rd gen 4th gen 5th gen 6th gen 7th gen 8th gen 9th gen 10th gen 11th gen M Xeon Nehalem-based Sandy Bridge-based Ivy Bridge-based Haswell-based Broadwell-based Skylake-based Other CISC iAPX 432 EPIC Itanium RISC i860 i960 StrongARM XScale Related Tick–tock model Process–architecture–optimization model Intel GPUs GMA Intel HD, UHD, and Iris Graphics Xe Arc PCHs SCHs ICHs PIIXs Stratix Codenames Larrabee Litigation Advanced Micro Devices, Inc. v. Intel Corp. High-Tech Employee Antitrust Litigation Intel Corp. v. Advanced Micro Devices, Inc. Intel Corp. v. Hamidi Intel Corporation Inc. v CPM United Kingdom Ltd Silvaco Data Systems v. Intel Corp. PeopleFounders Gordon Moore Robert Noyce CEOs Robert Noyce Gordon Moore Andrew Grove Craig Barrett Paul Otellini Brian Krzanich Bob Swan Pat Gelsinger Related Intel Foundation Achievement Award Mac transition to Intel processors Intel Architecture Labs ASCI Red BiiN Classmate PC Convera Corporation Copy Exactly! Intel Developer Forum Dynamic video memory technology Intel Extreme Masters List of Intel microprocessors List of Intel graphics processing units (2013 or earlier) I/O Acceleration Technology IA-32 Execution Layer IM Flash Technologies The Innovators Inside Films Inside The Beauty Inside The Power Inside Intel ADX Intel Capital Intel Cluster Ready Intel Compute Stick Intel Ireland Intel Mobile Communications Intel Outstanding Researcher Award Intel SHA extensions Intel Teach List of semiconductor fabrication plants List of Intel manufacturing sites List of mergers and acquisitions by Intel Intel Museum OnCue Intel PRO/Wireless Intel International Science and Engineering Fair Regeneron Science Talent Search Simple Firmware Interface Single-chip Cloud Computer Software Guard Extensions Supervisor Mode Access Prevention Tarari Intel Tera-Scale Timeline of Intel
vteIntel processorsLists Processors Atom Celeron Pentium Pro II III 4 D M Core 2 i3 i5 i7 i9 M Xeon Quark Itanium Microarchitectures Chipsets MicroarchitecturesIA-32 (32-bit x86) P5 P6 P6 variant (Pentium M) P6 variant (Enhanced Pentium M) NetBurst x86-64 (64-bit) Core Penryn Nehalem Westmere Sandy Bridge Ivy Bridge Haswell Broadwell Skylake Cannon Lake Sunny Cove Cypress Cove Willow Cove Golden Cove x86 ULV Bonnell Saltwell Silvermont Goldmont Goldmont Plus Tremont Gracemont Current productsx86-64 (64-bit) Atom Celeron Pentium Core 10th gen 11th gen 12th gen 13th gen 14th gen Xeon DiscontinuedBCD oriented (4-bit) 4004 (1971) 4040 (1974) pre-x86 (8-bit) 8008 (1972) 8080 (1974) 8085 (1977) Early x86 (16-bit) 8086 (1978) 8088 (1979) 80186 (1982) 80188 (1982) 80286 (1982) x87 (external FPUs) 8/16-bit databus 8087 (1980) 16-bit databus 80C187 80287 80387SX 32-bit databus 80387DX 80487 IA-32 (32-bit x86) i386 SX 376 EX i486 SX DX2 DX4 SL RapidCAD OverDrive A100/A110 Atom CE SoC Celeron (1998) M D (2004) Pentium Original i586 OverDrive Pro II III 4 M Dual-Core Core Xeon P6-based NetBurst-based Core-based Quark Tolapai x86-64 (64-bit) Atom SoC CE Celeron D Dual-Core Pentium 4 D Extreme Edition Dual-Core Core 2 1st gen 2nd gen 3rd gen 4th gen 5th gen 6th gen 7th gen 8th gen 9th gen 10th gen 11th gen M Xeon Nehalem-based Sandy Bridge-based Ivy Bridge-based Haswell-based Broadwell-based Skylake-based Other CISC iAPX 432 EPIC Itanium RISC i860 i960 StrongARM XScale Related Tick–tock model Process–architecture–optimization model Intel GPUs GMA Intel HD, UHD, and Iris Graphics Xe Arc PCHs SCHs ICHs PIIXs Stratix Codenames Larrabee
IA-32 (32-bit x86) P5 P6 P6 variant (Pentium M) P6 variant (Enhanced Pentium M) NetBurst x86-64 (64-bit) Core Penryn Nehalem Westmere Sandy Bridge Ivy Bridge Haswell Broadwell Skylake Cannon Lake Sunny Cove Cypress Cove Willow Cove Golden Cove x86 ULV Bonnell Saltwell Silvermont Goldmont Goldmont Plus Tremont Gracemont
x86-64 (64-bit) Atom Celeron Pentium Core 10th gen 11th gen 12th gen 13th gen 14th gen Xeon
BCD oriented (4-bit) 4004 (1971) 4040 (1974) pre-x86 (8-bit) 8008 (1972) 8080 (1974) 8085 (1977) Early x86 (16-bit) 8086 (1978) 8088 (1979) 80186 (1982) 80188 (1982) 80286 (1982) x87 (external FPUs) 8/16-bit databus 8087 (1980) 16-bit databus 80C187 80287 80387SX 32-bit databus 80387DX 80487 IA-32 (32-bit x86) i386 SX 376 EX i486 SX DX2 DX4 SL RapidCAD OverDrive A100/A110 Atom CE SoC Celeron (1998) M D (2004) Pentium Original i586 OverDrive Pro II III 4 M Dual-Core Core Xeon P6-based NetBurst-based Core-based Quark Tolapai x86-64 (64-bit) Atom SoC CE Celeron D Dual-Core Pentium 4 D Extreme Edition Dual-Core Core 2 1st gen 2nd gen 3rd gen 4th gen 5th gen 6th gen 7th gen 8th gen 9th gen 10th gen 11th gen M Xeon Nehalem-based Sandy Bridge-based Ivy Bridge-based Haswell-based Broadwell-based Skylake-based Other CISC iAPX 432 EPIC Itanium RISC i860 i960 StrongARM XScale
Founders Gordon Moore Robert Noyce CEOs Robert Noyce Gordon Moore Andrew Grove Craig Barrett Paul Otellini Brian Krzanich Bob Swan Pat Gelsinger
vteProgrammable logicConcepts ASIC SoC FPGA Logic block CPLD EPLD PLA PAL GAL PSoC Reconfigurable computing Xputer Soft microprocessor Circuit underutilization High-level synthesis Hardware acceleration Languages Verilog A AMS VHDL AMS VITAL SystemVerilog DPI SystemC AHDL Handel-C Lola PSL UPF PALASM ABEL CUPL OpenVera C to HDL Flow to HDL MyHDL ELLA Chisel Companies Accellera Achronix AMD Aldec Arm Cadence Infineon Intel Lattice Microchip Technology NXP Siemens Synopsys Texas Instruments ProductsHardware iCE Stratix Virtex Software Intel Quartus Prime Xilinx ISE Vivado ModelSim VTR Simulators IntellectualpropertyProprietary ARC ARM Cortex-M LEON LatticeMico8 MicroBlaze PicoBlaze Nios Nios II Open-source JOP LatticeMico32 OpenCores OpenRISC 1200 Power ISA Libre-SOC Microwatt RISC-V Zet
Hardware iCE Stratix Virtex Software Intel Quartus Prime Xilinx ISE Vivado ModelSim VTR Simulators IntellectualpropertyProprietary ARC ARM Cortex-M LEON LatticeMico8 MicroBlaze PicoBlaze Nios Nios II Open-source JOP LatticeMico32 OpenCores OpenRISC 1200 Power ISA Libre-SOC Microwatt RISC-V Zet
Proprietary ARC ARM Cortex-M LEON LatticeMico8 MicroBlaze PicoBlaze Nios Nios II Open-source JOP LatticeMico32 OpenCores OpenRISC 1200 Power ISA Libre-SOC Microwatt RISC-V Zet
Authority control databases ISNIVIAF

Infobox:
Altera CorporationHeadquarters in San Jose, CaliforniaCompany typeSubsidiaryIndustryIntegrated circuitsFoundedJune 1983; 41 years ago (June 1983)HeadquartersSan Jose, California, United StatesKey peopleSandra L. Rivera (CEO)ProductsFPGAsCPLDsEmbedded systems ASICsRevenue $1.932 billion (2014)Net income $472 million (2014)Total assets $5.674 billion (2014)Total equity $3.285 billion (2014)Number of employees3,091 (2014)ParentIntelWebsitealtera.comFootnotes / references


PDF Extracted Content:
Comprehensive Evaluation of OpenCL-based Convolutional Neural Network Accelerators in Xilinx and Altera FPGAs R. Tapiador, A. Rios-Navarro, A. Linares-Barranco. Robotic and Technology of Computers Lab. University of Seville Seville, SPAIN alinares@atc.us.es Minkyu Kim, Deepak Kadetotad, Jae-sun Seo. School of Electrical, Computer and Energy Engineering Arizona State University Tempe, AZ, USA jaesun.seo@asu.edu Abstract—Deep learning has significantly advanced the state of the art in artificial intelligence, gaining wide popularity from both industry and academia. Special interest is around Convolutional Neural Networks (CNN), which take inspiration from the hierarchical structure of the visual cortex, to form deep layers of convolutional operations, along with fully connected classifiers. Hardware implementations of these deep CNN architectures are challenged with memory bottlenecks that require many convolution and fully-connected layers demanding large amount of communication for parallel computation. Multi- core CPU based solutions have demonstrated their inadequacy for this problem due to the memory wall and low parallelism. Many-core GPU architectures show superior performance but they consume high power and also have memory constraints due to inconsistencies between cache and main memory. FPGA design solutions are also actively being explored, which allow implementing the memory hierarchy using embedded BlockRAM. This boosts the parallel use of shared memory elements between multiple processing units, avoiding data replicability and inconsistencies. This makes FPGAs potentially powerful solutions for real-time classification of CNNs. Both Altera and Xilinx have adopted OpenCL co-design framework from GPU for FPGA designs as a pseudo-automatic development solution. In this paper, a comprehensive evaluation and comparison of Altera and Xilinx OpenCL frameworks for a 5- layer deep CNN is presented. Hardware resources, temporal performance and the OpenCL architecture for CNNs are discussed. Xilinx demonstrates faster synthesis, better FPGA resource utilization and more compact boards. Altera provides multi-platforms tools, mature design community and better execution times. Keywords—Deep Learning; Convolutional Neural Network; Hardware Acceleration; OpenCL; FPGA; Caffe; Xilinx; Altera. I. INTRODUCTION In recent years, throughout a series of breakthrough algorithms , convolutional neural networks significantly improved the state-of-the-art in large-scale image recognition tasks. Driven by such success, CNNs have become widespread across a broad range of applications including vision, object detection, speech recognition, autonomous driving, image captioning, etc. Typically CNNs consists of a large number of deep layers, and could involve hundreds of millions of parameters. Using high-end GPGPUs (General Purpose Graphic Processing Units), the networks are trained iteratively using back-propagation algorithm for days or weeks, and then the networks with trained weights can be deployed onto hardware for classification tasks. There has been a number of prior works that built hardware on different platforms for efficient CNN implementation (as accelerators or complete architecture on hardware), such as FPGA and ASIC (application-specific integrated circuits) . ASIC or custom chip designs show better energy-efficiency, but may not flexibly map various CNN algorithms easily with the rigid circuits. On the other hand, FPGA platforms are much more flexible and could easily map any given CNN algorithm with hardware optimizations. For FPGAs, the designers could perform manual RTL designs , but using high-level synthesis tools could prove effective in terms of design time and wide design space exploration. The authors in employed HLS tools in Xilinx framework to optimize CNN implementation, while the authors in explored Open Computing Language (OpenCL) based implementation in Altera framework for throughput optimization of CNNs. Since the high-level synthesis tools are developed differently within different frameworks of Xilinx and Altera, it is difficult to determine which option or FPGA chip would be the best candidate for certain objectives (area, speed, etc.) from the designer’s point of view. In this paper, we provide a comprehensive evaluation and comparison of the same CNN using both Xilinx and Altera’s OpenCL-based high-level synthesis tool flows. The remainder of the paper is organized as follows. In Section II, the OpenCL programming and models are described. In Section III, Altera’s OpenCL design flow and hardware system is discussed, while Xilinx’s SDAaccel design flow and hardware platform is presented in Section IV. LeNet-5 ConvNet for MNIST database digits classification scenario is presented in Section V. In Section VI, the hardware results and implementation are compared between the two designs from different vendors in a comprehensive manner. The paper is concluded in Section VII. II. OPENCL FOR FPGA Parallel computing has considerably improved in the last years thanks to the technology scaling favors. From single core CPUs and DSP (Digital Signal Processors), well oriented to single-instruction-multiple-data (SIMD) vectored architectures, computing market changed to multicore chips in the early year 2k when Intel and AMD started to manufacture them. Nevertheless, Rockwell International manufactured the first dual core chip with its version of the 6.502 with two cores in the eighties , sharing the chip’s pins on alternate clock phases. DSP architecture are oriented to speed up the signal processing using floating point units. Parallelism is obtained thanks to well oriented memory hierarchy and SIMD, very- long-instruction-words (VLIW) and superscalar architectures to maximize the instructions-per-cycle ratio (IPC). In the past decade, parallelism improvement started to be oriented to multi-core architectures for general purpose computers, or many-core to more specific solutions, as GPGPUs. For example, the Tesla K80 accelerator has 4,992 cores with a dual-GPU design that allows up to 2.9 double precision TFLOPS or 8.73 single-precision TFLOPS . Special interest has existed for the FPGAs and SoC framework that includes programmable logic cells oriented to embedded co- design solutions. Recent FPGA technology is highly competitive allowing ASIC emulation with a considerably high resource necessity, such as Stratix 10 (14nm TriGate process, 5.5M Logic Elements, up to 23 TMACS and 10 TFLOPS) or Virtex UltraScale+ (16nm process, 3.7M Logic Cells and up to 21 TMACs) . To implement a given CNN model onto FPGA hardware, we start from the publicly available codes in the Caffe framework . The input image for the CNN model is converted to a text file from Python interface in Caffe and the text file is read from OpenCL host code. Using the Python interface in Caffe, both the input data and weights are extracted and fed to the OpenCL host code, on a batch of input images. The hardware implementation computes till the last inner product layer output and compares it to the expected output from Caffe, to ensure correct functionality. Typically, the CNN models in Caffe are realized using double-precision values for the nodes and the weights. Considering efficient hardware implementation, we first find out how much precision reduction could be achieved while having minimal degradation in the final classification accuracy, and this reduced precision will be used when the OpenCL codes are written. OpenCL is an open royalty-free standard for general- purpose parallel programming across heterogeneous platforms . Through a programming interface, OpenCL will form a foundation layer of a parallel computing ecosystem of platform-independent tools, middleware and applications. It is very well oriented to graphics rendering pipelines but it is increasing the interest of the FPGA community. OpenCL consists of an application-programming-interface (API) that coordinates parallel computation across heterogeneous parallel processors under the same platform. It has a programming language with a specified computation environment that supports both data and task-based parallel programming models. Therefore, OpenCL is a framework that includes a language, API, libraries and a runtime system to support software development. Four different models define the core ideas behind OpenCL: A. Platform Model The OpenCL platform consists of a host computer connected to several devices. Each OpenCL device is divided into compute units (CUs). Each CU is divided into processing elements (PE), where computations occur (Fig. 1). The OpenCL application is implemented as both host code and device kernel code. Each part will run in their specific hardware. The host code submits each kernel code as commands from the host to PEs through the memory hierarchy. When PEs of a CU execute the same sequence of instructions, the control flow is called to be converged. In this case, single instructions over multiple PEs occurs, which is in fact the same concept of SIMD. If one PE needs a different sequence of instructions, then the control flow is called to be diverged. In OpenCL, converged and diverged control flows may occur in the same kernel, providing great flexibility. B. Execution Model. OpenCL has two units of execution: kernels that execute in one or more platforms, and a host program that executes on a host computer. Kernels execute the computation through work- items (with an associated ID), which are executed in groups (work-groups). The context of what the kernel executes is defined by the host. The host program uses the OpenCL API to create and manage the context. This API has a set of functions that enable the host interaction with devices through a command-queue. There are three main commands: kernel (to order the kernel execution), memory (data transfer between host and devices) and synchronization (synchronize points for order definition between commands). When a kernel-enqueue command submits a kernel for execution, an index space is defined. This index space is called NDRange in OpenCL, which corresponds to an N-dimensional index space. N is 1, 2 or 3. The NDRange is decomposed into work-groups forming blocks. It is defined by three integer arrays: global size (the extent of the index space in each dimension), an offset index (initial value of indices in each dimension), and the local-size (size of the work-group in each dimension). C. Memory Model There are four different memory regions in OpenCL for work-item execution. Global memory is where all work-items of all work-groups have to read and write data. These accesses must be cached. Constant memory is a region that remains without changes during the kernel execution. The host initializes this memory. Local memory is the one used by work- groups locally. It is shared by all work-items. It can be mapped into regions of the global memory. Private memory is a memory region that is only visible for a work-item, such that any other work-item cannot access this memory of a particular work-item. Data flow between memory regions is controlled Fig. 1: OpenCL platform model. through commands that the host enqueues. The memory consistency is guaranteed in a work-item and between a work- group and all its work-items, but there is no guarantee of memory consistency between different work-groups executing a kernel. D. Parallel Computation Considerations In OpenCL, there are mainly two ways to parallelize a kernel: (1) using multiple compute-units (CUs) in parallel (see Fig. 3, left), and (2) vectorising data processing through SIMD kernels with a unique CU (see Fig. 3, right). When multiple CUs are used in parallel, a kernel is replicated and the replicated kernels work simultaneously, increasing throughput and, therefore, consuming global memory bandwidth and hardware resources. On the other hand, SIMD vectorization increases throughput by vectoring kernels, which allows processing multiple work items in a single instruction (SIMD). This alternative is more efficient than using several CUs because it only duplicates the data paths. In this paper, SIMD experiments are presented because the use of replicated CUs generates global memory bottlenecks due to many parallel accesses during the execution. Fig. 2: Multiple parallel CU (left) versus single CU with SIMD (right). E. Programming Model There are two supported programming models: data parallel and task parallel. The data parallel model defines a computation as a sequence of instructions applied to multiple elements of a memory. On the other hand, the task parallel model requires the kernel to be executed in a single work-item of a work-group. In this case, several kernels can be executed in parallel. Synchronization is possible between work-items of a work-group or through two types of enqueued commands: barrier (it ensures all previous commands have been executed) or wait-on-an-event (the command to be executed waits for a particular event in memory before executing itself). III. SCENARIO 1: ALTERA OPENCL Altera OpenCL (AOCL) is a framework for developing host applications that send kernels to be executed in parallel in FPGA resources. Work-groups, their work-items and memory models are implemented automatically in FPGA resources from an OpenCL description of the kernels and a C++ host application calling API libraries for different functionalities, such as: set buffers, call kernels, synchronize through events and read results. AOCL allows users to abstract the traditional hardware FPGA development flow and instead work with a much faster and higher-level software development flow. Using this design flow, it is possible to emulate OpenCL code in a FPGA, generating synthesis report files as timing or resources summaries. The design flow consists of two parts: host software application and kernel accelerator hardware on FPGA. The concept is that host sends data to the kernels, where complex calculations are accelerated. A. Design Flow The design flow for an Altera board using OpenCL consists of several steps. The first step is to describe the functionality of the kernels using C/C++ and then to optimize each kernel applying OpenCL directives to generate a *.cl file. In addition, a host application must be written in C/C++ using the recommended environment. B. Hardware Platform The implementation of the LeNet-5 CNN for MNIST handwritten digit recognition has been developed on a Terasic DE5-Net (see Fig. 2). This board supports Altera OpenCL and its main characteristics include up to 8 GB DDR3 RAM memory running at 800MHz, 72Mb SRAM running at 550MHz, PCIe-x8 and Altera Stratix V GX-5SGXEA7N- 2F45C2 FPGA, which features are shown in Table 1. Table 1. Altera Stratix V-GXA7 Specs Logic Elements (K) 622 M20K (Blocks / Mbits) 2560 / 50 18-bit × 18-bit Multipliers 512 27-bit × 27-bit Multipliers 256 IV. SCENARIO 2: XILINX SDACCEL The SDAccel development environment is a command line based tool suite for compiling OpenCL programs into a Xilinx FPGA device. The design flow is similar to AOCL in terms of host and kernel descriptions. Directives and API must be replaced when same project is developed for both vendor environments. SDAccel is only available for RedHat Linux OS and it works in a batch mode, where the user invokes the tool with a command file. These commands allow to define the solution name, adding the target device (only one per solution) and host files, creating the kernels and adding the files where they are implemented, creating the Xilinx OpenCL compute unit binary file, and building and packaging the systems. Several CUs per kernel can be implemented. Each CU can have several PEs, which emulates the SIMD architecture. One Fig. 2: Terasic DE5-Net board. PCIe x8 Altera Stratix V FPGA DDR3 800MHz SODIMM sockets (8GB) important advantage over the Altera tool is that SDAccel lets the programmer to test the application before compiling and generating the FPGA binary file. A disadvantage is that SDAccel is less mature than AOCL. A. CPU and Hardware Emulation SDAccel allows emulating in CPU the codesign program before building the system for FPGA. These methods are called CPU and Hardware emulation. The CPU emulation is typically used for functional verification. Each kernel of our solution is compiled into CUs that is executed as a thread on CPU. Hardware emulation is slower since it uses a hardware simulator, but this emulation reproduces the final design on FPGA. The main advantage of using hardware emulation is to avoid the long implementation times (8h on average for this work). Table 2: Virtex 7 XC7VX690T features. Physical Characteristics Logic Cells 693120 Configurable logic blocks Slices 108300 Max distributed RAM (Kb) 10888 DSP Slices 3600 Block RAM (blocks/Kb) 2940/52920 B. Building the System This flow builds the system in the real hardware of the target device. When the compilation/implementation is completed, a number of files have been created to run the application, such as the executable, the Xilinx OpenCL binary container (*.xclbin) and the FPGA programming file. For these experiments, the Alpha Data ADM-PCIE-7V3 board (see Fig. 4) has been used under the CentOS 6.6 operating system. This board is most powerful board that supports SDAccel. The main features include two 8GB ECC- SODIMM memory up to 1333MT/s (faster than Altera DE5 platform), one PCI Express Gen3 x8 and Xilinx Virtex 7 XC7VX690T-2FFG1157C. The features of the FPGA are listed in Table 2. Besides the DSP slices, the specification of the FPGA is similar to that of Altera’s Stratix V-GXA7. V. LENET-5 AND MNIST SCENARIO LeNet-5 CNN architecture (shown in Fig. 5) serves as the baseline for many recent CNN-based classification algorithms. It combines three architectural ideas to ensure a certain degree of shift, scale and distortion invariance: local receptive fields, shared weights and spatial sub-sampling. The input layer represents a size-normalized and centered image. In this case, the size corresponds to the size of MNIST database digits (28x28). The first layer is the result of a set of convolutions over the input image. Each pixel in C1 receives inputs from a set of units located in a small neighborhood of the previous layer. This represents the kernel of the convolution. In this exercise, size of the kernel is 5x5. This operation mimics the locally-sensitive, orientation-selective neurons in the cat’s visual system, discovered by Hube and Wiesel . These receptive fields in neurons are able to learn and extract elementary visual features, such as edges, end- points, and corners. The combination of these features by subsequent layers are able to detect higher-order features. C1 in this example extracts 20 features from the input image. S2 performs a sub-sampling operation of local averaging, reducing the resolution of the feature maps where distinctive features are encoded. Typically, these convolution and sub-sampling layers are sequentially instantiated for feature map combinations. They are implemented in a bi-pyramid way: at each layer, the number of feature maps is increased as the spatial resolution is decreased. C3 is a convolution layer for 50 smaller feature maps and S4 is the corresponding sub-sampling layer that performs the same operation as that in S2. C3 combines all of the S2 features. The last layer of this CNN is a fully-connected classifier with 500 input neurons and 10 output neurons, which also includes a Rectification Linear Unit (ReLU). VI. COMPARISON STUDY The implementation of this Le-Net5 using the OpenCL framework impose some restrictions. Fig. 5 (bottom) shows the block diagram of the OpenCL solution. It can be seen that the host application, running on a computer, sends input images and kernel weights to the logic through PCIe interface. Data is then stored in the DDR memory in the platform, called “Global Memory”. This memory is continuously and iteratively accessed by the logic (FPGA) through all the parallel devices physically implemented in hardware. The CNN is structured in 5 kernels (stages), where first kernel implements first layer convolutions and their subsampling operations (conv_pool1); second kernel performs the second layer convolutions (conv2), which is more complex since it has to take results from 20 instances of previous layer, and perform convolutions for 50 instances of this second layer. Then, the third kernel implements the second subsampling operations (pool2). The forth kernel has 500 instances for the classifier unit, whose inputs are the outputs of the previous 50 instances (ip1_relu). Each of these 500 devices send their output to a final layer with 10 instances (one per digit) to categorize the winner digit in the classification (ip2). Each of these devices read the global memory, process the corresponding operation, and then write back the results to the global memory. Consecutive kernels (stages or layers of the CNN) execute in-order, which are controlled by special events included during OpenCL compilations. This architecture needs a high bandwidth DDR memory interface to support all required parallel instances. OpenCL can implement each kernel in a replicated manner as many times in parallel as possible, or it can execute one after the other sequentially if no parallelism can be implemented. As Fig. 4: Alpha-Data ADM-PCIE-7V3 board. more parallelism is employed, the global memory behavior worsens. The main difference between Altera and Xilinx platforms is the DDR3 on-board memory speed (800MHz for Altera and 1333MHz for Xilinx) as mentioned previously. OpenCL allows other memory implementations to avoid this shared memory bottleneck, like local pipes that connect two devices directly in the logic. Each of these pipes is implemented through small FIFOs as a point-to-point communication channel between two devices. For CNNs, these pipes do not represent a feasible solution because internal convolution layers, such as C3 in this case, have to read all the S2 outputs and combine them into each of the 50 C3 outputs. This represents 50 pipes at C3 per for each of the 20 S2 units, which is not viable, in terms of resource consumption, for the selected platforms. Therefore, we selected the global memory interface as the possible solution to work for both platforms, and we provide a comprehensive comparison. Three different tests have been developed for these platforms with the same CNN described in the previous section. The first test consists of comparing each FPGA executing each layer of the CNN without any kind of parallelism. The second test aims to do same measurements when loops are unrolled. For the last test, SIMD directives have been included to vectorise each layer. Table 3 shows the results of these three experiments for the two vendor platforms. Execution times, logic resources, the number of DSP units and block RAM that are used are shown per kernel. In general, execution time is improved upon employing more parallelism up to a limit. The limit occurs due to the bottleneck that the global memory accesses impose. As expected, the usage of logic gates and DSP units increases when parallelism is increased. Altera tools are able to extract much more parallelism than Xilinx, as it can be seen on logic elements /cells and DSP utilization. There are very small differences between unrolling and SIMD for both platforms for this experiment. Altera tool is able to extract more aggregation for SIMD than Xilinx. In fact, for Xilinx, both unrolling and SIMD have almost same results. Table 3: Test results: No parallelism / Unroll / SIMD Kernel Name Execution Time (ms) Logic Cells /Elem. (K) DSP slices BRAM (Kb) Xilinx Virtex 7 690T conv_ pool1 3.63/1.96/1.96 4.9/6.2/5.1 11/11/11 180/216/ 216 conv2 7.62/4.92/4.92 4.8/4.8/4.9 11/11/11 108/144/ 144 pool2 0.03/0.06/0.06 3.0/4.0/3.0 4/4/4 72/144/144 ip1_ relu 0.55/0.55/0.55 4.2/4.2/4.2 11/11/11 72/72/72 ip2 0.35/0.35/0.35 4.0/3.0/4.0 9/9/9 72/72/72 Altera Stratix V GXA-7 conv_ pool1 1.01/1.01/0.98 145.7/42.3/73.7 8/31/57 5225/6205/ 11200 conv2 3.95/3.96/4.27 300.5/34.0/34.0 8/31/31 3207/4882/ 4900 pool2 0.08/0.07/0.13 6.9/6.9/6.8 2/2/2 279/273/ 279 ip1_ relu 1.01/1.81/2.02 5.8/5.8/5.8 4/4/4 1471/1470/ 1500 ip2 0.15/0.14/0.13 5.7/5.7/5.7 4/4/4 1471/1470/ 1500 Table 4: Acceleration comparison Kernel name Xilinx vs Altera Acceleration % Acceleration Conv_pool1 3,59/1,94/2 259 / 94 / 100 Conv2 1,92/1,24/1,15 92 / 24 / 15 Pool2 -2,66/1,16/2,16 - 166 / -16 / -116 Ip1_relu -1,83/3,29/3,67 -83 / -229 / -267 Ip2 2,33/2,5/2,69 133 / 150 / 169 In order to demonstrate the different DDR memory bandwidth limits of these two platforms, the same real-time experiment has been performed in both platforms. The experiment consists of connecting a webcam to the host application, which continuously reads in an image frame, normalizing it and resizing to 28x28 pixels using OpenCV libraries. The host sends kernels parameters in the beginning and then it iterates the process of acquiring an image frame, pre-processing it, sending it to the platform and checking the final classification results. The on-board DDR in the Altera platform could not support the memory bandwidth required by this demonstration and the time per frame is continuously increasing. In contrast, Xilinx platform supported this real-time experiment owing to the higher DDR bandwidth. Results show that time increase when parallelism is applied. This is due memory bandwidth when multiple access to global memory are done. Bottlenecks slow down kernel increasing execution time. In general, logic elements, DSP and BRAM have increased when parallelized directives are applied. However, Fig. 5: LeNet-5 ConvNet architecture (top) for MNIST digit recognition (middle) and its OpenCL based hardware block diagram (bottom). the time does not get better due to bottleneck generated by DDR memory bandwidth. Table 4 represents the acceleration between vendors. Execution times for Xilinx are much better than Altera except for pool2 and ip1_relu stages. VII. REAL TIME BEHAVIOUR The previous experiment does not test the behavior against a continuous dataflow of images. In this section a real time experiment has been developed with both platforms (see Fig.6). The experiment uses the OpenCV library in the host computer to capture frames and send them to the FPGA continuously. Altera platform for this particular MNIST experiment, showed a DDR bandwidth bottleneck (800 MHz), which implied a continuously increasing frame processing time, because of the stacked global memory accesses. On the other hand, Xilinx platform didn’t reached this DDR bandwidth bottleneck and needed time per frame was constant (DDR memory is 1,3GHz). Nevertheless, Xilinx global cycle time per frame was higher (table 3), but constant. Fig 6: OpenCV Application VIII. CONCLUSIONS This work presents a comparison between two OpenCL FPGA-based platforms (Altera and Xilinx) executing a convolutional neural network. Results show that the Altera platform has better execution time for each kernel than the Xilinx platform for all test scenarios. However, the Xilinx platform requires less FPGA resources than the Altera counterpart to execute the same CNN model. The real-time experiment developed for both platforms has demonstrated that the DDR memory bandwidth is crucial for the global memory communication architecture. Other memory architectures, such as pipes, are implemented internally to the FPGA without requiring any off-chip memory bandwidth, but it was insufficient for CNNs because of their point-to-point restriction. A new memory model that allows having double- buffered memory spread on the FPGA blockRAM will avoid the bottlenecks identified in this work. This will allow having more CUs in parallel to further improve the performance. ACKNOWLEDGMENT This work has been partially supported by Samsung Advance Institute of Technology, and by Xilinx and Altera University Programs, through platform donations. REFERENCES A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet Classification with Deep Convolutional Neural Networks,” in Advances in Neural Information Processing Systems, 2012. K. Simonyan and K. Zisserman, “Very deep convolutional networks for large-scale image recognition,” in International Conference on Learning Representations (ICLR), 2015. C. Szegedy, et al., “Going Deeper With Convolutions,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015. S. Ioffe and C. Szegedy, “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,” in International Conference on Machine Learning (ICML), 2015. K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image Recognition,” arXiv:1512.03385, 2015. C. Farabet, et al., “Hardware accelerated convolutional neural networks for synthetic vision systems,” in IEEE International Symposium on Circuits and Systems (ISCAS), 2010. V. Gokhale, et al., “A 240 G-ops/s Mobile Coprocessor for Deep Neural Networks,” in IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2014. C. Zhang, et al., “Optimizing FPGA-based accelerator design for deep convolutional neural networks,” in ACM International Symposium On Field-Programmable Gate Arrays (FPGA), 2015. N. Suda, V. Chandra, G. Dasika, A. Mohanty, Y. Ma, S. Vrudhula, J. Seo, and Y. Cao, “Throughput-Optimized OpenCL-based FPGA Accelerator for Large-Scale Convolutional Neural Networks,” in ACM International Symposium on Field-Programmable Gate Arrays (FPGA), 2016. Y. Chen, T. Luo, S. Liu, S. Zhang, L. He, J. Wang, L. Li, T. Chen, Z. Xu, N. Sun, and O. Temam, “DaDianNao: A Machine-Learning Supercomputer,” in IEEE/ACM International Symposium on Microarchitecture (MICRO), 2014. Y-H. Chen, T. Krishna, J. Emer, and V. Sze, "Eyeriss: An Energy- Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks," in IEEE International Solid-State Circuits Conference (ISSCC), 2016. J. Sim, J-S. Park, M. Kim, D. Bae, Y. Choi, and L-S. Kim, "A 1.42TOPS/W Deep Convolutional Neural Network Recognition Processor for Intelligent IoE Systems," in IEEE International Solid-State Circuits Conference (ISSCC), 2016. Y. Jia, et al., “Caffe: Convolutional architecture for fast feature embedding,” in ACM International Conference on Multimedia, 2014. Rockwell International, “Rockwell R65C00/21 Dual CMOS Microcomputer and R65C29 Dual CMOS Microprocessor,” October 1984. Michael Parker, “Understanding Peak Floating-Point Performance Claims”, Technical White Paper WP-012220-1.0, June 2014, Altera Corporation. UltraScale Architecture and Product Overview, DS890(v2.7), February 2016, Xilinx. “The OpenCL Specification Version: 2.0,” Khronos OpenCL Working Group Editors: Lee Howes and Aaftab Munshi. https://www.khronos.org/ Xilinx SDAccel development environment user guide. http://www.xilinx.com/products/design-tools/software-zone/sdaccel.html Alpha Data ADM-PCIE-7V3 user manual. http://www.alpha- data.com/pdfs/adm-pcie-7v3%20user%20manual.pdf Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document recognition,” Proceedings of the IEEE, vol. 86, no. 11, pp. 2278-2324, Nov. 1998. D. H. Hubel and T. N. Wiesel, “Receptive fields, binocular interaction, and functional architecture in the cat’s visual cortex,” Journal of Physiology (London), vol. 160, pp. 106-154, 1962. Tesla K80 GPU Accelerator, Board Specification BD_07317_001_V05, January 2015. http://images.nvidia.com/content/pdf/kepler/Tesla-K80- BoardSpec-07317-001-v05.pdf
